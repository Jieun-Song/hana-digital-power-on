# Hugging Face Datasets overviews(Pytorch)

[https://www.youtube.com/watch?v=_BZearw7f0w&t=30s](https://www.youtube.com/watch?v=_BZearw7f0w&t=30s)

í—ˆê¹…í¼ì´ìŠ¤ ë°ì´í„°ì…‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” public datasetì„ ë¹ ë¥´ê²Œ ë‹¤ìš´ë¡œë“œí•˜ë„ë¡ APIë¥¼ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. 

### ë‹¤ìš´ë¡œë“œ ë°©ë²• & Preprocess a dataset

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

load_dataset í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ íŒŒë¼ë¯¸í„°ë¡œ ë„˜ê²¨ì§„ identifierë¥¼ í†µí•´ directly ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.

ìœ„ ì½”ë“œì—ì„œëŠ” GLUE ë²¤ì¹˜ë§ˆí¬ì—ì„œ MRPC ë°ì´í„°ì…‹ì„ fetch í–ˆë‹¤.

í•´ë‹¹ ë°ì´í„°ì…‹ì€ ì˜ì—­ì„ í•˜ëŠ”(?) sentence ì§ì„ ê°€ì§€ê³ ìˆë‹¤.

- paraphrases(ì˜ì—­)
    
    ì›ë˜ì˜ ë¬¸ì¥ì´ë‚˜ êµ¬ì ˆì„ ì‰½ê²Œ í’€ì–´ë‚´ëŠ” ê²ƒ
    
    ì˜ë¯¸ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë‹¤ë¥¸ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´ì–¸ê°€ë¥¼ ì“°ê±°ë‚˜ ë§í•˜ëŠ” ê²ƒ.(ë¬¸ì¥ì„ ë” ê°„ë‹¨í•˜ê³ , ë” ì§§ê³ , ë” ëª…í™•í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©ë¨)
    
    ì‚¬ìš©í•˜ëŠ” ì´ìœ  : ì›ë³¸ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì´í•´/ ì¸ìš©ì— ëŒ€í•œ ëŒ€ì•ˆ..
    

```python
DatasetDict({
	train: Dataset({
		features: ['sentence1', 'sentence2', 'label', 'idx'],
		num_rows: 3668
	})
	validation: Dataset({
		features: ['sentence1', 'sentence2', 'label', 'idx'],
		num_rows: 408
	})
	test: Dataset({
		features: ['sentence1', 'sentence2', 'label', 'idx'],
		num_rows: 1725
	})
})
```

load_dataset í•¨ìˆ˜ë¡œ ë°˜í™˜ë˜ëŠ” objectëŠ” DatasetDictë¡œ, ë°ì´í„°ì…‹ splitì„ í¬í•¨í•˜ê³ ìˆëŠ” dictionaryì§‘í•©ì´ë‹¤.

ê°ê°ì˜ splitì€ splitì˜ ì´ë¦„ì„ indexí•´ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

```python
raw_datasets["train"]
```

```python
Dataset({
		features: ['sentence1', 'sentence2', 'label', 'idx'],
		num_rows: 3668
})
	
```

ì´ splitì€ Dataset í´ë˜ìŠ¤ì˜ instanceì´ë©°, 'sentence1', 'sentence2', 'label', 'idx' í–‰ê³¼ 3668ê°œì˜ ì—´ì„ ê°–ê³ ìˆë‹¤.

ê°ê°ì˜ ì¸ë±ìŠ¤ë¡œ elementì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤.

```python
row_datasets["train"][6]
```

```python
{'idx' : 6,
 'label' : 0,
 'sentence1' : 'The Nasdaq blah blah',
 'sentence2' : 'The technique blah blah'}
```

Hugging Face Datasets Libraryì˜ ì¢‹ì€ ì ì€ Apache Arrowë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì´ ë””ìŠ¤í¬ì— ì €ì¥ëœë‹¤ëŠ” ì ì´ë‹¤. 

- Apache Arrow
    
    ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì¸í”„ë¼ê°€ ì„œë¡œ ê°„ì˜ ë°ì´í„° ê³µìœ ë¥¼ ìœ„í•´ APIë¥¼ ì´ìš©í•  ë–„ ë°œìƒí•˜ëŠ” ë¬¸ì œì  ì¤‘ í•˜ë‚˜ëŠ” ì§ë ¬í™”ì™€ ì—­ì§ë ¬í™”ì˜ overheadê°€ ë„ˆë¬´ ë†’ë‹¤ëŠ” ê²ƒì´ë‹¤(ì• í”Œë¦¬ì¼€ì´ì…˜ ì„±ëŠ¥ğŸ‘‡ğŸ»)
    
    ArrowëŠ” ì–¸ì–´, í”Œë«í¼ê³¼ ìƒê´€ì—†ì´ ë©”ëª¨ë¦¬ ìƒì—ì„œ ì»¬ëŸ¼ êµ¬ì¡°ë¡œ ë°ì´í„°ë¥¼ ì •ì˜í•˜ì—¬, CPUì™€ GPUì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ë¹ ë¥´ê²Œ ì½ê³  ì“¸ ìˆ˜ ìˆë„ë¡ í•œë‹¤.
    
    [https://realsalmon.tistory.com/21](https://realsalmon.tistory.com/21)
    
    ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-08-10 á„‹á…©á„’á…® 2.39.49.png](Hugging%20Face%20Datasets%20overviews(Pytorch)%20c42fa8d285a549a4989edcea5bc46409/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-08-10_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.39.49.png)
    
    ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-08-10 á„‹á…©á„’á…® 2.40.00.png](Hugging%20Face%20Datasets%20overviews(Pytorch)%20c42fa8d285a549a4989edcea5bc46409/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-08-10_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.40.00.png)
    

ë§Œì•½ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” RAMì˜ ìš©ëŸ‰ë³´ë‹¤ datasetì´ ë” í¬ë‹¤ê³  í•œë‹¤í•´ë„, ìš°ë¦¬ê°€ ìš”ì²­í•˜ëŠ” elementsë§Œì´ ë©”ëª¨ë¦¬ì— ì €ì¥ëœë‹¤.

ìš°ë¦¬ì˜ dataset sliceë¥¼ ì ‘ê·¼í•˜ëŠ” ê²ƒì€ í•˜ë‚˜ì˜ elementì— ì ‘ê·¼í•˜ëŠ” ê²ƒë§Œí° ì‰½ë‹¤. ì´í›„ ê²°ê³¼ëŠ” ê° í‚¤ì˜ ê°’ì˜ ì‚¬ì „ í˜•íƒœë¡œ ë‚˜ì˜¨ë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-08-10 á„‹á…©á„’á…® 1.49.48.png](Hugging%20Face%20Datasets%20overviews(Pytorch)%20c42fa8d285a549a4989edcea5bc46409/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-08-10_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_1.49.48.png)

Datasetì˜ featuresì†ì„±ì€ ìš°ë¦¬ì—ê²Œ í–‰ì— ëŒ€í•œ ë” ë§ì€ ì •ë³´ë¥¼ ì¤€ë‹¤. 

```python
raw_datasets["train"].features
```

```python
{'sentence1':Value(dtype="string", id = None),
 'sentence2':Value(dtype="string", id = None),
 'label' : ClassLabel(num_classes = 2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx' : Value(dtype= 'int32', id=None)}
```

ì´ë ‡ê²Œ ë°ì´í„° íƒ€ì…ì´ë‚˜ ë¨¸,,, ê·¸ëŸ°ê±° ì•Œë ¤ì¤€ë‹¤.

ì •ìˆ˜ë“¤ê³¼ ë¼ë²¨ë“¤ ì´ë¦„ ì‚¬ì´ì˜ ìƒì‘í•˜ëŠ” ê²ƒì„ ì•Œìˆ˜ìˆë‹¤.

ì—¬ê¸°ì„œ 0ì€ not_equivalentë¥¼, 1ì€ equivalentë¥¼ ì˜ë¯¸í•œë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-08-10 á„‹á…©á„’á…® 2.01.00.png](Hugging%20Face%20Datasets%20overviews(Pytorch)%20c42fa8d285a549a4989edcea5bc46409/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA_2024-08-10_%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE_2.01.00.png)

ìš°ë¦¬ Datasetì˜ ëª¨ë“  elementsë¥¼ ë¯¸ë¦¬ ì •ì œí•´ ë†“ê¸° ìœ„í•´ì„  í† í°í™”(tokenize)ë¥¼ í•´ì•¼í•œë‹¤.

- ì´ ì˜ìƒ ì°¸ê³ í•˜ëœë‹¤,,
    
    Preprocessing sententce pairs
    
    [https://www.youtube.com/watch?v=0u3ioSwev3s](https://www.youtube.com/watch?v=0u3ioSwev3s)
    

tokenizerì— additional keyword argumentì™€ í•¨ê»˜ ë‘ ë¬¸ì¥ì„ ë³´ë‚´ì•¼í•œë‹¤. (ë„ˆë¬´ ê¸´ inputë“¤ì€ ì˜ë¼ì§„ë‹¤(truncate). )

ìš°ë¦¬ëŠ” ì§ì ‘ì ìœ¼ë¡œ datasetì— ëª¨ë“  splitì„ ë”í•˜ëŠ” tokenize_functionì— map methodì™€ í•¨ê»˜ ëª¨ë“  ê²ƒì„ ë„£ëŠ”ë‹¤.

```python
from transformers import AutoTokenizer

checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
	return tokenizer(
		example["sentence1"], example["sentence2"], padding="max_length", truncation=True, max_length = 128
	)

tokenized_datasets = raw_datasets.map(tokenize_function)
print(tokenized_datasets.column_names)
```

functionì´ dictionaryì™€ ê°™ì€ í˜•íƒœë¥¼ returní•´ì£¼ê¸° ë–„ë¬¸ì—, map methodëŠ” ìƒˆë¡œìš´ í–‰ì„ ë”í•˜ê±°ë‚˜ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í–‰ì„ updateí•˜ëŠ” ì—­í• ì„ í• ê²ƒì´ë‹¤.

```python
from transfomers import AutoTokenizer

checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(examples):
	return tokenizer(
		examples["sentence1"], examples["sentence2"], padding="max_length",
			truncation=True, max_length=128
		)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
#batched=True!ë¥¼ ì´ìš©í•´ ì•„ë˜ ê¸°ëŠ¥ ìˆ˜í–‰ê°€ëŠ¥
```

preprocessingë¥¼ ë” ë¹ ë¥´ê²Œ í•˜ê³ , í† í¬ë‚˜ì´ì €ê°€ Rustê¸°ë°˜ì´ë¼ëŠ” ê²ƒì˜ ì¥ì ì„ ì‚´ë ¤, ìš°ë¦¬ëŠ” ì—¬ëŸ¬ elementë¥¼ ë™ì‹œì— ì²˜ë¦¬í• ìˆ˜ìˆë‹¤.(batched=True argumentë¥¼ ì´ìš©í•´..)

tokenizerëŠ” first/second sentenceì˜ listë¥¼ ë‹¤ë£° ìˆ˜ ìˆê¸° ë•Œë¬¸ì— tokenize_functionì€ ì´ê²ƒì„ ìœ„í•œ ë³€ê²½ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤. ë˜í•œ map methodì™€ í•¨ê¼ ë©€í‹°í”„ë¡œì„¸ì‹±ì„ ì‚¬ìš©í• ìˆ˜ì‡ë‹¤.

```python
tokenized_datasets = tokenized_datasets.remove_columns(["idx","sentence1","sentence2"])
tokenized_datasets = tokenized_datasets.rename_columns(["label","labels")
tokenized_datasets = tokenized_datasets.with_format("torch")
tokenized_datasets["train"]
```

```python
Dataset({
		features: ['attention_mask', 'input_ids', 'labels', 'token_type_ids'],
		num_rows: 3668
})
```